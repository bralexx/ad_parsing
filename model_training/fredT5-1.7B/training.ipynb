{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../utils')\n",
    "from json_format import process_json, unprocess_json\n",
    "from evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = 'ai-forever/FRED-T5-1.7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "converters = {'json': json.loads}\n",
    "train = pd.read_csv('../data/train_9k_valid.csv', index_col=0, converters=converters).sample(frac=1, random_state=42)\n",
    "val_set = pd.read_csv('../data/val_set_300_sb_valid.csv', index_col=0, converters=converters).sample(frac=1, random_state=42)\n",
    "manual_test = pd.read_csv('../data/manual_test_100.csv', index_col=0, converters=converters).sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens([\"<BOB>\", \"<EOB>\", \"<BOT>\", \"<EOT>\", \"<BOP>\", \"<EOP>\", \"<BOC1>\", \"<EOC1>\", \"<BOC2>\", \"<EOC2>\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "assert train.loc[train.index[0], 'json'] == unprocess_json(process_json(train.loc[train.index[0], 'json']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'json', '__index_level_0__'],\n",
       "        num_rows: 8766\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text', 'json', '__index_level_0__'],\n",
       "        num_rows: 45\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ads_dataset = Dataset.from_pandas(train[[\"Text\", \"json\"]])\n",
    "ads_dataset = ads_dataset.train_test_split(test_size=0.005, seed=42)\n",
    "ads_dataset = ads_dataset.flatten()\n",
    "ads_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37635369a3f7475f96de6b32739f7c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/8766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4005: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4005: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4005: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4005: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53d631b62654a8280b829dca3c243fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4005: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4005: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4005: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4005: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [tokenizer.bos_token + text + tokenizer.eos_token for text in examples[\"Text\"]]\n",
    "    targets = [tokenizer.bos_token + process_json(bundles) + tokenizer.eos_token for bundles in examples[\"json\"]]\n",
    "    # model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True)\n",
    "    # return model_inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "ads = ads_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ads_dataset[\"train\"].column_names\n",
    ")\n",
    "ads = ads.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricComputer:\n",
    "  def __init__(self, batch_size=8):\n",
    "    self.batch_size = batch_size\n",
    "    self.generations = []\n",
    "\n",
    "  def __call__(self, eval_preds):\n",
    "    ev = Evaluator(val_set, model=model, tokenizer=tokenizer, batch_size=self.batch_size, seq_tokens=True)\n",
    "    stats = ev.calc_bleu_batched()\n",
    "    self.generations.append(ev.generate_samples_batched(count=20))\n",
    "    # clear_output()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/training_args.py:1493: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 8\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir=\"fredT5-1.7B-checkpoints\",\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=6000,\n",
    "  learning_rate=4e-5,\n",
    "  per_device_train_batch_size=2,\n",
    "  per_device_eval_batch_size=2,\n",
    "  weight_decay=0.01,\n",
    "  save_total_limit=1,\n",
    "  save_steps=6000,\n",
    "  num_train_epochs=n_epochs,\n",
    "  lr_scheduler_type=\"cosine\",\n",
    "  group_by_length=False,\n",
    "  warmup_steps=3,\n",
    "  # load_best_model_at_end= True,\n",
    ")\n",
    "\n",
    "\n",
    "mc = MetricComputer(batch_size=2)\n",
    "empty_dataset = Dataset.from_dict({\"Text\": [], \"json\": []})\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=ads[\"train\"],\n",
    "  eval_dataset=ads[\"test\"],\n",
    "  # eval_dataset=empty_dataset,\n",
    "  tokenizer=tokenizer,\n",
    "  data_collator=data_collator,\n",
    "  compute_metrics=mc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18290' max='35064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18290/35064 3:42:08 < 3:23:45, 1.37 it/s, Epoch 4.17/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bep-sb</th>\n",
       "      <th>Bep-multi</th>\n",
       "      <th>Ta-bleu-sb</th>\n",
       "      <th>Ta-chrf-sb</th>\n",
       "      <th>Ta-chrf-multi</th>\n",
       "      <th>Eb-ind</th>\n",
       "      <th>Mb-ind</th>\n",
       "      <th>Bleu-classic</th>\n",
       "      <th>Chrf-classic</th>\n",
       "      <th>Chrf-classic-multi</th>\n",
       "      <th>Bleu Old</th>\n",
       "      <th>Failed Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.284800</td>\n",
       "      <td>0.247434</td>\n",
       "      <td>0.709373</td>\n",
       "      <td>0.672864</td>\n",
       "      <td>40.696003</td>\n",
       "      <td>79.140267</td>\n",
       "      <td>78.811325</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>56.647225</td>\n",
       "      <td>76.165868</td>\n",
       "      <td>75.451996</td>\n",
       "      <td>56.647225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.224835</td>\n",
       "      <td>0.688866</td>\n",
       "      <td>0.664838</td>\n",
       "      <td>37.400845</td>\n",
       "      <td>76.927791</td>\n",
       "      <td>77.063078</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>52.830884</td>\n",
       "      <td>73.503712</td>\n",
       "      <td>74.160753</td>\n",
       "      <td>52.830884</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.156600</td>\n",
       "      <td>0.196096</td>\n",
       "      <td>0.724390</td>\n",
       "      <td>0.700108</td>\n",
       "      <td>41.650682</td>\n",
       "      <td>79.768090</td>\n",
       "      <td>80.311817</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>57.357349</td>\n",
       "      <td>77.024426</td>\n",
       "      <td>78.672094</td>\n",
       "      <td>57.357349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOT>—á–µ—à—Å–∫–∏–π –±–∏—Å–µ—Ä<EOT><BOP>1<EOC1><BOC2>EUR<EOC2>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35065' max='35064' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35064/35064 2:06:51, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bep-sb</th>\n",
       "      <th>Bep-multi</th>\n",
       "      <th>Ta-bleu-sb</th>\n",
       "      <th>Ta-chrf-sb</th>\n",
       "      <th>Ta-chrf-multi</th>\n",
       "      <th>Eb-ind</th>\n",
       "      <th>Mb-ind</th>\n",
       "      <th>Bleu-classic</th>\n",
       "      <th>Chrf-classic</th>\n",
       "      <th>Chrf-classic-multi</th>\n",
       "      <th>Bleu Old</th>\n",
       "      <th>Failed Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.205346</td>\n",
       "      <td>0.728746</td>\n",
       "      <td>0.706940</td>\n",
       "      <td>41.445821</td>\n",
       "      <td>79.507208</td>\n",
       "      <td>79.609243</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>57.040565</td>\n",
       "      <td>76.961041</td>\n",
       "      <td>77.369716</td>\n",
       "      <td>57.040565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Could not locate the best model at fredT5-1.7B-checkpoints/checkpoint-24000/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'fredT5-1.7B-checkpoints/checkpoint-24000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bralex/.venv/lib/python3.10/site-packages/transformers/trainer.py:1912\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bralex/.venv/lib/python3.10/site-packages/transformers/trainer.py:2403\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_total_limit \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2402\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m checkpoint \u001b[38;5;129;01min\u001b[39;00m checkpoints_sorted:\n\u001b[0;32m-> 2403\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_model_checkpoint\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2404\u001b[0m             logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeleting older checkpoint [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] due to args.save_total_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2405\u001b[0m             shutil\u001b[38;5;241m.\u001b[39mrmtree(checkpoint)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/genericpath.py:101\u001b[0m, in \u001b[0;36msamefile\u001b[0;34m(f1, f2)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether two pathnames reference the same actual file or directory\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mThis is determined by the device number and i-node number and\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mraises an exception if an os.stat() call on either pathname fails.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m s1 \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(f1)\n\u001b[0;32m--> 101\u001b[0m s2 \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samestat(s1, s2)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fredT5-1.7B-checkpoints/checkpoint-24000'"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d5c64a86c841b184fdc901627f4fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = './checkpoint-30000'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:588: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BEP-sb': 0.7287457256519236,\n",
       " 'BEP-multi': 0.7069398307496324,\n",
       " 'TA-BLEU-sb': 41.44582147695938,\n",
       " 'TA-CHRF-sb': 79.50720754462834,\n",
       " 'TA-CHRF-multi': 79.60924297907755,\n",
       " 'EB-ind': 0.016,\n",
       " 'MB-ind': 0.046,\n",
       " 'BLEU-classic': 57.040564511240156,\n",
       " 'CHRF-classic': 76.96104108112667,\n",
       " 'CHRF-classic-multi': 77.36971623947342,\n",
       " 'bleu_old': 57.040564511240156,\n",
       " 'failed_ratio': 0.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = Evaluator(manual_test, model=model, tokenizer=tokenizer, batch_size=2)\n",
    "output = ev.generate_samples_batched()\n",
    "df = pd.DataFrame([{'id': i, 'json': json.dumps(v, indent=4, ensure_ascii=False)} for i, v in output.items()])\n",
    "df.to_csv('manual_test_outputs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fredT5-1.7B-trained/tokenizer_config.json',\n",
       " 'fredT5-1.7B-trained/special_tokens_map.json',\n",
       " 'fredT5-1.7B-trained/vocab.json',\n",
       " 'fredT5-1.7B-trained/merges.txt',\n",
       " 'fredT5-1.7B-trained/added_tokens.json',\n",
       " 'fredT5-1.7B-trained/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "output_dir = \"fredT5-1.7B-trained\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad/bralex/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1244: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad><s> –ø—Ä–æ–¥–∞–º<s> –ø—Ä–æ–¥–∞–º<s> –ø—Ä–æ–¥–∞–º<s> –ø—Ä–æ–¥–∞–º<s> –ø—Ä–æ–¥–∞–º<s> –ø—Ä–æ–¥–∞–º<s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s = '<s> –ø—Ä–æ–¥–∞–º –∑–æ–Ω—Ç –∑–∞ 400 —Ä—É–±–ª–µ–π</s>'\n",
    "\n",
    "# inputs = tokenizer(s, return_tensors='pt').to('cuda')\n",
    "# out = model.generate(**inputs)\n",
    "# tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>–∫—Ä–æ–≤–∞—Ç—å –∏–∫–µ—è –æ–¥–Ω–æ—Å–ø–∞–ª—å–Ω–∞—è —Ä–∞—Å—Ç—É—â–∞—è, –æ—Ç–ª–∏—á–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ 70‚Ç¨ –ª–∏–º–∞—Å—Å–æ–ª</s>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.decode(ads['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(output_dir).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distill_data = pd.read_csv('../data/distill_data.csv', index_col=0)\n",
    "# distill_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ev = Evaluator(distill_data)\n",
    "# output = ev.generate_samples_batched(count=20000, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
